{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading input data\n",
    "data = np.genfromtxt('./data/tt.csv', delimiter=',')\n",
    "y = data[:,0].reshape(-1,1)\n",
    "X = data[:, 1:]\n",
    "m = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some network architecture\n",
    "input_layer_size  = 784\n",
    "hidden_layer_size = 28\n",
    "num_labels = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 shape: (28, 785)\n",
      "theta2 shape: (29, 26)\n",
      "0.020924158377765553\n",
      "-0.02026313823109456\n"
     ]
    }
   ],
   "source": [
    "# Creating parameters and a flattened theta\n",
    "_lambda = 0\n",
    "theta1 = np.random.rand(hidden_layer_size,785) * 2 * 0.11 - 0.11 #dimension(hidden layer size, input layer size + 1)\n",
    "theta2 = np.random.rand(hidden_layer_size + 1,num_labels) * 2 * 0.11 - 0.11  #dimension(hiddent layer size + 1, labels)\n",
    "flat_theta1 = (theta1.T).ravel()\n",
    "flat_theta2 = (theta2.T).ravel()\n",
    "nn_params = np.concatenate((flat_theta1, flat_theta2))\n",
    "\n",
    "#debug stuff\n",
    "print(f'theta1 shape: {np.shape(theta1)}')\n",
    "print(f'theta2 shape: {np.shape(theta2)}')\n",
    "print(theta2[10,10])\n",
    "print(theta2[20,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels ):\n",
    "    theta1_elems = ( input_layer_size + 1 ) * hidden_layer_size\n",
    "    theta1_size  = ( input_layer_size + 1, hidden_layer_size  )\n",
    "    theta2_size  = ( hidden_layer_size + 1, num_labels )\n",
    "\n",
    "    theta1 = nn_params[:theta1_elems].T.reshape( theta1_size ).T\n",
    "    theta2 = nn_params[theta1_elems:].T.reshape( theta2_size ).T\n",
    "    return theta1, theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda):\n",
    "    # Reshape nn params back into size\n",
    "    theta1, theta2 = param_unroll(nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "    print(np.shape(theta1))\n",
    "    print(np.shape(theta2))\n",
    "    print(theta2[10,10])\n",
    "    print(theta2[20,6])\n",
    "\n",
    "    # forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 785)\n",
      "(26, 29)\n",
      "0.08562544454841618\n",
      "-0.003983345853868295\n"
     ]
    }
   ],
   "source": [
    "J = cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
