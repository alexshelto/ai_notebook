{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize, special\n",
    "import itertools\n",
    "from tqdm import trange\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Format might be wrong! X_train.reshape(X_train.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights for theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rand_initialize_weights(layer_in, layer_out):\n",
    "#     # gaussian is strong\n",
    "#     # uniform is stronger\n",
    "#     ret = np.random.uniform(-1., 1., size=(layer_out,layer_in))/np.sqrt(layer_out * layer_in)\n",
    "#     return ret.astype(np.float32)\n",
    "\n",
    "\n",
    "def rand_initialize_weights(layer_in, layer_out):\n",
    "    # gaussian is strong\n",
    "    # uniform is stronger\n",
    "    ret = np.random.uniform(-1., 1., size=(layer_out,layer_in + 1))/np.sqrt(layer_out * layer_in)\n",
    "    return ret.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 784)\n",
      "examples: 688\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# train_data = np.genfromtxt('./data/sign_mnist_train.csv', delimiter=',')\n",
    "# test_data = np.genfromtxt('./data/sign_mnist_test.csv', delimiter=',')\n",
    "train_data = np.genfromtxt('./data/tt.csv', delimiter=',')\n",
    "y = train_data[:,0].reshape(-1,1)  # Shape: (m,1)\n",
    "X = train_data[:, 1:] / 255        # Shape: (m, input_layer_size)\n",
    "# X = X.reshape(-11, 28, 28)\n",
    "print(np.shape(X))\n",
    "\n",
    "\n",
    "m = len(y)\n",
    "print(f'examples: {m}')\n",
    "\n",
    "#(688, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 shape: (150, 785), theta2 shape: (26, 151)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8873302340>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_layer_size = 784\n",
    "hidden_layer_size = 150      # hidden layer has 120 nodes excluding bias \n",
    "num_labels = 26              # 26 total output values: a=0, b=1, ... , z=25\n",
    "\n",
    "\n",
    "np.random.seed(1999)\n",
    "theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)\n",
    "theta2 = rand_initialize_weights(hidden_layer_size, num_labels)\n",
    "\n",
    "print(f'theta1 shape: {np.shape(theta1)}, theta2 shape: {np.shape(theta2)}')\n",
    "\n",
    "# supposed to be: : theta1 shape (120, 785), theta2 shape: (26, 121)\n",
    "plt.imshow(X[3].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function and derivative\n",
    "* Used for activation function in forward propogation\n",
    "* Derivative used for back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "   # return (1 / (1 + np.exp(-z)))\n",
    "    s = expit(z)\n",
    "    return s\n",
    "\n",
    "def dx_sigmoid(z):\n",
    "    res = sigmoid(z)\n",
    "    return (res * (1 - res))\n",
    "\n",
    "print(sigmoid(0))\n",
    "print(dx_sigmoid(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_labeled(y, num_labels):\n",
    "#     '''returns (m,num_labels) matrix where each column corresoponds to an example\n",
    "#     all indexes are 0, each column contains a single 1, the row corresponding to the answer'''\n",
    "#     m = np.size(y)\n",
    "#     out = np.zeros((num_labels, m), np.float32)\n",
    "#     for i in range(0, m):\n",
    "#         label = int(y[i])\n",
    "#         out[label, i] = 1\n",
    "#     return out\n",
    "\n",
    "\n",
    "def return_labeled(y, num_labels):\n",
    "    '''returns (m,num_labels) matrix where each column corresoponds to an example\n",
    "    all indexes are 0, each column contains a single 1, the row corresponding to the answer'''\n",
    "    m = np.size(y)\n",
    "    out = np.zeros((m, num_labels), np.float32)\n",
    "    for row in range(0, m):\n",
    "        label = int(y[row])\n",
    "        out[row, label] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_backward(X,y):\n",
    "#     print(f'X shape: {np.shape(X)}')\n",
    "#     m = len(y)\n",
    "#     # Forward propogation\n",
    "# #     ones = np.ones((m,1))\n",
    "# #     a1 = np.hstack((ones,X)) # m, input_layer + 1\n",
    "# #     z2 = theta1.dot(a1.T) \n",
    "# #     a2 = np.c_[np.ones((X.shape[0],1)),sigmoid(z2.T)] \n",
    "# #     z3 = theta2.dot(a2.T) # 10x26 * 26x5000 = 26xm \n",
    "# #     a3 = sigmoid(z3)\n",
    "#     a1 = np.c_[np.ones((m,1)), X]   # layer 1 : adding a bias column of 1's to X. (m, input_size + 1)\n",
    "#     z2 = a1.dot(theta1.T)           # layer 2 matrix calculation\n",
    "#     a2 = sigmoid(z2)                # activation of layer 2: shape(m, hidden_layer_size)\n",
    "#     a2 = np.c_[np.ones((m, 1)), a2] # adding a bias column of 1's : shape(m, hidden_layer_size + 1)\n",
    "#     z3 = a2.dot(theta2.T)           # layer3 || output layer calculation\n",
    "#     a3 = sigmoid(z3)                # activation of output layer: size = (m, num_labels)\n",
    "    \n",
    "#     #Cost function\n",
    "# #     inner_term0 = (-y_k * np.log(a3))\n",
    "# #     inner_term1 = (1 - y_k) * np.log(1 - a3)\n",
    "# #     left_side = np.sum(inner_term0 - inner_term1) / m\n",
    "# #     right_side = np.sum(theta1[:, 1:] ** 2) + np.sum(theta2[:,1:] ** 2) # sum of all theta vals squred excluding theta index0 \n",
    "# #     right_side = (lam / 2 / m) * right_side\n",
    "# #     J = left_side + right_side\n",
    "#     cost = -1*(1/m)*np.sum((np.log(a3.T)*(y_k)+np.log(1-a3).T*(1-y_k)))+(lam/(2*m))*(np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))\n",
    "        \n",
    "\n",
    "                           \n",
    "#     # Back propogation\n",
    "# #     delta3 = a3 - y_k\n",
    "# #     delta2 = delta3.dot(theta2)[:,1:] * dx_sigmoid(z2) # excluding bias\n",
    "# #     Gradient1 = (delta2.T).dot(a1)\n",
    "# #     Gradient2 = (delta3.T).dot(a2)\n",
    "\n",
    "#     d3 = a3.T - y_k # 5000x10\n",
    "#     d2 = theta2[:,1:].T.dot(d3.T)*dx_sigmoid(z2) # 25x10 *10x5000 * 25x5000 = 25x5000\n",
    "    \n",
    "#     delta1 = d2.dot(a1) # 25x5000 * 5000x401 = 25x401\n",
    "#     delta2 = d3.T.dot(a2) # 10x5000 *5000x26 = 10x26\n",
    "    \n",
    "#     Theta1_ = np.c_[np.ones((theta1.shape[0],1)),theta1[:,1:]]\n",
    "#     Theta2_ = np.c_[np.ones((theta2.shape[0],1)),theta2[:,1:]]\n",
    "    \n",
    "#     Gradient1 = delta1/m + (Theta1_*lam)/m\n",
    "#     Gradient2 = delta2/m + (Theta2_*lam)/m\n",
    "    \n",
    "# #     print(f'Gradient 1: {Gradient1}')\n",
    "# #     print(f'Gradient 2: {Gradient2}')\n",
    "\n",
    "#     return a1, a2, a3, cost, Gradient1, Gradient2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is still hitting the fan remove bias'\n",
    "\n",
    "def forward_backward(theta1, theta2, X, y):\n",
    "    labels = return_labeled(y, num_labels)\n",
    "    m = len(y)\n",
    "    # Feed forward propogation\n",
    "    a1 = np.c_[np.ones((m,1)), X]   # layer 1 : adding a bias column of 1's to X. (m, input_size + 1)\n",
    "    z2 = a1.dot(theta1.T)           # layer 2 matrix calculation\n",
    "    a2 = sigmoid(z2)                # activation of layer 2: shape(m, hidden_layer_size)\n",
    "    a2 = np.c_[np.ones((m, 1)), a2] # adding a bias column of 1's : shape(m, hidden_layer_size + 1)\n",
    "    z3 = a2.dot(theta2.T)           # layer3 || output layer calculation\n",
    "    a3 = sigmoid(z3)                # shape(m, num_labels)\n",
    "    \n",
    "    # Calculating cost (MSE)\n",
    "    cost = ((1 / 2) * (np.power((a3 - labels), 2)))\n",
    "    print(cost.sum())\n",
    "    \n",
    "    ################################################\n",
    "    zh = z2\n",
    "    ah = a2\n",
    "    ao = a3\n",
    "    zo = z3\n",
    "    wo = theta1\n",
    "    wo = theta2\n",
    "    \n",
    "    dcost_dao = ao - labels\n",
    "    dao_dzo = dx_sigmoid(zo) \n",
    "    dzo_dwo = ah\n",
    "\n",
    "    dcost_wo = np.dot(dzo_dwo.T, dcost_dao * dao_dzo)\n",
    "\n",
    "    # Phase 2 =======================\n",
    "\n",
    "    # dcost_w1 = dcost_dah * dah_dzh * dzh_dw1\n",
    "    # dcost_dah = dcost_dzo * dzo_dah\n",
    "    dcost_dzo = dcost_dao * dao_dzo\n",
    "    dzo_dah = wo\n",
    "    dcost_dah = np.dot(dcost_dzo , dzo_dah)\n",
    "    dah_dzh = dx_sigmoid(zh) \n",
    "    dzh_dwh = X\n",
    "\n",
    "    # dzh_dwh & dah_dzh : (hidden layer size, input layer size)\n",
    "    dcost_wh = dzh_dwh.T.dot( (dah_dzh * dcost_dah[:,1:]) )\n",
    "#     # Back propogation loss\n",
    "#     d3 = a3 - labels                    #dcost_dao\n",
    "#     d3_dx = dx_sigmoid(a3)              #dao_dzo\n",
    "#     dcost_wo = a2.T.dot(d3 * d3_dx)       #dcost_wo\n",
    "#     # Back propogation gradients \n",
    "#     d2_cost = d3 * d3_dx                #dcost_dzo\n",
    "#     # wo=theta2\n",
    "#     dcost_dah = d2_cost.dot(theta2)\n",
    "#     dah_dzh = dx_sigmoid(a2)\n",
    "#     dcost_wh = labels.T.dot(dah_dzh * dcost_dah)\n",
    "    \n",
    "    return cost, dcost_wh.T, dcost_wo.T\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 4.37 accuracy 0.07: 100%|██████████| 1/1 [00:00<00:00, 194.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391.53654516917425\n",
      "grad1: (150, 784)\n",
      "grad2: (26, 151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 120\n",
    "lam = 3 # Lambda used for regularization in cost function\n",
    "\n",
    "\n",
    "# reinitialize thetas\n",
    "theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)\n",
    "theta2 = rand_initialize_weights(hidden_layer_size, num_labels)\n",
    "\n",
    "losses, accuracies = [], []\n",
    "\n",
    "# # Model training\n",
    "epochs = 1000\n",
    "mmm = 0\n",
    "for i in (t := trange(epochs)):\n",
    "    samp = np.random.randint(0, X.shape[0], size=(batch_size))\n",
    "    X_train = X[samp]\n",
    "    y_train = y[samp]\n",
    "    \n",
    "    cost, Gradient1, Gradient2 = forward_backward(theta1, theta2, X_train, y_train)\n",
    "    \n",
    "    print(f'grad1: {np.shape(Gradient1)}')\n",
    "    print(f'grad2: {np.shape(Gradient2)}')\n",
    "\n",
    "    \n",
    "    theta1 = theta1[:,1:] - (learning_rate * Gradient1)\n",
    "    theta2 = theta2 - (learning_rate * Gradient2)\n",
    "    \n",
    "    prediction = np.argmax(h_x, axis=1)\n",
    "    \n",
    "    loss = J.mean()\n",
    "    losses.append(loss)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    t.set_description(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Model training\n",
    "# epochs = 1000\n",
    "# mmm = 0\n",
    "# for i in (t := trange(epochs)):\n",
    "#     samp = np.random.randint(0, X.shape[0], size=(batch_size))\n",
    "#     X_train = X[samp]\n",
    "#     y_train = y[samp]\n",
    "    \n",
    "#     for example in range(0, batch_size):\n",
    "        \n",
    "#         a1, a2, h_x, J, grad1, grad2 = forward_backward(theta1, theta2,X_train[example], y_train[example])\n",
    "    \n",
    "#         prediction = np.argmax(h_x, axis=1)\n",
    "    \n",
    "#         accuracy = (prediction == y_train).mean()\n",
    "#         if (accuracy > mmm):\n",
    "#             mmm = accuracy\n",
    "    \n",
    "#         theta1 -= (learning_rate * (grad1))\n",
    "#         theta2 -= (learning_rate * (grad2))\n",
    "    \n",
    "#         loss = J.mean()\n",
    "#         losses.append(loss)\n",
    "    \n",
    "#         accuracies.append(accuracy)\n",
    "#     t.set_description(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
    "\n",
    "# print(f'max accuracy: {mmm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
