{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(L_in, L_out):\n",
    "    e = 0.12 # sigma\n",
    "    t = np.random.random((L_out, L_in + 1)) * 2 * e - e\n",
    "    return t\n",
    "\n",
    "\n",
    "def recode_label(y,num_labels):\n",
    "    m = np.shape(y)[0]\n",
    "    out = np.zeros( ( num_labels, m ) ) #(num_labels, m)\n",
    "    for i in range(0, m):\n",
    "        out[int(y[i]-1), i] = 1\n",
    "\n",
    "    return out\n",
    "    #     rows = len(y)\n",
    "#     out = np.zeros((rows, num_labels))\n",
    "#     for i in range(0, rows):\n",
    "#         row_answer = int(y[i])\n",
    "#         out[i, row_answer] = 1\n",
    "        \n",
    "#     return out\n",
    "\n",
    "def param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels ):\n",
    "    '''\n",
    "    theta1 shape: (30, 785) (hidden_size, input_size + 1)\n",
    "    theta2 shape: (26, 31)  (num_labels, hidden_size + 1)\n",
    "    '''\n",
    "    theta1_elems = ( input_layer_size + 1 ) * hidden_layer_size\n",
    "    theta1_size  = ( input_layer_size + 1, hidden_layer_size  )\n",
    "    theta1 = nn_params[:theta1_elems].T.reshape( theta1_size ).T\n",
    "\n",
    "    \n",
    "\n",
    "    theta2_size  = ( hidden_layer_size + 1, num_labels )\n",
    "    theta2 = nn_params[theta1_elems:].T.reshape( theta2_size ).T\n",
    "\n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Some math\n",
    "\n",
    "def sigmoid(z):\n",
    "    return ( (1 / (1 + np.exp(-z))) )\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return (sigmoid(z) * (1 - sigmoid(z)))\n",
    "\n",
    "print(sigmoid(0.0)) #should return 0.5\n",
    "print(sigmoid_gradient(0.0)) # should return 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(theta1, theta2, X, X_bias=None):\n",
    "    '''\n",
    "    a1 = (m, input_layer_size + 1), a2 = (m, hidden_layer_size + 1), a3= (m, num_labels)\n",
    "    theta1 = (hidden_layer_size, input_layer_size + 1)\n",
    "    theta2 = (num_labels, hidden_layer_size)\n",
    "    '''\n",
    "    one_rows = np.ones((1, np.shape(X)[0] ))\n",
    "    a1 = np.r_[one_rows, X.T]  if X_bias is None else X_bias\n",
    "    z2 = theta1.dot( a1 )\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.r_[one_rows, a2] \n",
    "    z3 = theta2.dot( a2 )\n",
    "    a3 = sigmoid( z3 )\n",
    "#     # Input layer\n",
    "#     m, _ = np.shape(X)\n",
    "#     one_rows = np.ones((1, np.shape(X)[0] ))\n",
    "#     a1 = np.c_[np.ones((m,1)), X] # assigning a1 to X, and adding a bias (m, input_layer_size + 1)\n",
    "#     # Hidden layer\n",
    "#     z2 = a1.dot(theta1.T)\n",
    "#     a2 = sigmoid(z2)\n",
    "#     a2 = np.c_[np.ones((np.shape(a2)[0], 1)), a2] # bias for hidden layer\n",
    "#     # Output layer\n",
    "#     z3 = a2.dot(theta2.T)\n",
    "#     a3 = sigmoid(z3) #a3 = h(x)\n",
    "    \n",
    "    return (a1, a2, a3, z2, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda, yk = None, X_bias = None ):\n",
    "    \n",
    "    theta1, theta2 = param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "    a1,a2,a3,z2,z3 = feed_forward(theta1, theta2, X, X_bias)\n",
    "    \n",
    "    if yk is None:\n",
    "        yk = recode_label(y, num_labels)\n",
    "        assert shape(yk) == shape(a3), \"Error, shape of recoded y is different from a3\"\n",
    "    \n",
    "    # J(theta) function: cross-entropy\n",
    "    term1 = (-y_k * np.log(a3))\n",
    "    term2 = (1 - y_k) * np.log(1 - a3)\n",
    "    cost = np.sum(term1 + term2)/m\n",
    "    # Regularization sum\n",
    "    reg_term = np.sum(theta1 ** 2) + np.sum(theta2[:,1:] ** 2)\n",
    "    reg_term = (_lambda/2/m) * reg_term\n",
    "    return(cost + reg_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda, yk = None, X_bias = None ):\n",
    "   \n",
    "    m, n = np.shape(X)\n",
    "    theta1, theta2 = param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "    a1, a2, a3, z2, z3 = feed_forward(theta1, theta2, X)\n",
    "    \n",
    "\n",
    "    # back propagate\n",
    "    if yk is None:\n",
    "        yk = recode_label(y, num_labels )\n",
    "        assert shape(yk) == shape(a3), \"Error: shape of recoded y is different from a3\"\n",
    "        \n",
    "    delta_3 = a3 - y_k #(m, num_labels), theta2=(labels, hidden_label size)\n",
    "    delta_2 = (delta_3.dot(theta2))[:,1:] * sigmoid_gradient(z2) #ignore bias\n",
    "    sum_2 = delta_3.T.dot(a2) # sum of a_i * delta_i+1\n",
    "    sum_1 = delta_2.T.dot(a1)\n",
    "    \n",
    "    # putting the gradient equation together\n",
    "    theta_2_grad = (sum_2[:,1:] / m) + ((theta2[:,1:] * _lambda) / m)\n",
    "    theta_1_grad = (sum_1[:,1:] / m) + (theta1[:,1:] * _lambda / m)\n",
    "    \n",
    "    # Sizes\n",
    "    #grad1: (28, 784)\n",
    "    #grad2: (26, 28)\n",
    "    print(f'grad1 shape: {np.shape(theta_1_grad)}, grad2 shape: {np.shape(theta_2_grad)}')\n",
    "    grad_flat = np.r_[theta_1_grad.T.flatten(), theta_2_grad.T.flatten()]\n",
    "    return grad_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_k: (26, 5)\n",
      "(785, 5)\n"
     ]
    }
   ],
   "source": [
    "# loading input data\n",
    "data = np.genfromtxt('./data/5.csv', delimiter=',')\n",
    "y = data[:,0].reshape(-1,1)\n",
    "X = data[:, 1:] #(m, input_layer_size)\n",
    "m = len(y)\n",
    "\n",
    "# Network architecture \n",
    "input_layer_size = 784\n",
    "hidden_layer_size = 30\n",
    "num_labels = 26\n",
    "lam = 1.0\n",
    "\n",
    "# Params\n",
    "theta1 = initialize_weights( 784, 30 ) # (input_size, hidden layer 1 size)\n",
    "theta2 = initialize_weights( 30, 26 )  # (hidden layer 1 size, # labels)\n",
    "unrolled = np.r_[theta1.T.flatten(), theta2.T.flatten()] # 1 dimension (24356,)\n",
    "\n",
    "#grad1 shape: (30, 784) \n",
    "#grad2 shape: (26, 30)\n",
    "\n",
    "\n",
    "# matrix holding correct values\n",
    "y_k = recode_label(y,num_labels)\n",
    "print(f'shape of y_k: {np.shape(y_k)}')\n",
    "\n",
    "X_bias = np.r_[ np.ones((1, np.shape(X)[0] )), X.T] #(input_size + 1, m)\n",
    "print(np.shape(X_bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debugging the parameters before\n",
    "# print(f'rolled params: {np.shape(unrolled)}')\n",
    "# print(f'number of elements in theta1: {np.size(theta1)}')\n",
    "# print(f'number of elements in theta2: {np.size(theta2)}')\n",
    "# print(f'theta1 shape: {np.shape(theta1)}')\n",
    "# print(f'theta2 shape: {np.shape(theta2)}')\n",
    "\n",
    "# print('='*20)\n",
    "\n",
    "# t1,t2 = param_unroll(unrolled, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "# # Debugging after:\n",
    "# print(f'number of elements in theta1: {np.size(t1)}')\n",
    "# print(f'number of elements in theta2: {np.size(t2)}')\n",
    "# print(f'theta1 shape: {np.shape(t1)}')\n",
    "# print(f'theta2 shape: {np.shape(t2)}')\n",
    "\n",
    "# # debugging values are the same\n",
    "# print(f'{theta1[0,0] == t1[0,0]},{theta1[12,25] == t1[12,25]},{theta1[11, 200] == t1[11,200]}')\n",
    "# print(f'{theta2[0,0] == t2[0,0]},{theta2[12,25] == t2[12,25]},{theta2[11, 11] == t2[11,11]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.263152075438711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-a233d91c06c3>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return ( (1 / (1 + np.exp(-z))) )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feed_forward(theta1,theta2, X, X_bias)\n",
    "print(compute_cost(unrolled,input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias))\n",
    "\n",
    "result = optimize.fmin_cg(compute_cost, fprime=compute_gradient, x0=unrolled,\n",
    "    args=(input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias),\n",
    "    maxiter=50, disp=True, full_output=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
