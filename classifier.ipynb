{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize, special\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(L_in, L_out):\n",
    "    dt = np.dtype('f8')   # 64-bit floating-point number\n",
    "    t = np.zeros((L_out, L_in + 1), dtype=dt)\n",
    "    e = 0.12 # sigma\n",
    "    t[:] = np.random.randn(*t.shape) * 2.0 * e - e\n",
    "    \n",
    "    print(f'generated theta: {np.shape(t)}')\n",
    "#     t = np.random.random((L_out, L_in + 1)) * 2.0 * e - e\n",
    "    return t\n",
    "\n",
    "\n",
    "def recode_label(y,num_labels):\n",
    "    m = np.shape(y)[0]\n",
    "    out = np.zeros( ( num_labels, m ) ) #(num_labels, m)\n",
    "    for i in range(0, m):\n",
    "        out[int(y[i]-1), i] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels ):\n",
    "    '''\n",
    "    theta1 shape: (30, 785) (hidden_size, input_size + 1)\n",
    "    theta2 shape: (26, 31)  (num_labels, hidden_size + 1)\n",
    "    '''\n",
    "    theta1_elems = ( input_layer_size + 1 ) * hidden_layer_size\n",
    "    theta1_size  = ( input_layer_size + 1, hidden_layer_size  )\n",
    "    theta1 = nn_params[:theta1_elems].T.reshape( theta1_size ).T\n",
    "\n",
    "    \n",
    "\n",
    "    theta2_size  = ( hidden_layer_size + 1, num_labels )\n",
    "    theta2 = nn_params[theta1_elems:].T.reshape( theta2_size ).T\n",
    "\n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Some math\n",
    "\n",
    "def sigmoid(z):\n",
    "    return special.expit(z)\n",
    "    #return ( (1 / (1 + np.exp(-z))) )\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    sig = sigmoid(z)\n",
    "    return (sig * (1 - sig))\n",
    "   #return (sigmoid(z) * (1 - sigmoid(z)))\n",
    "\n",
    "print(sigmoid(0.0)) #should return 0.5\n",
    "print(sigmoid_gradient(0.0)) # should return 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(theta1, theta2, X, X_bias=None):\n",
    "    '''\n",
    "    a1 = (m, input_layer_size + 1), a2 = (m, hidden_layer_size + 1), a3= (m, num_labels)\n",
    "    theta1 = (hidden_layer_size, input_layer_size + 1)\n",
    "    theta2 = (num_labels, hidden_layer_size)\n",
    "    '''\n",
    "    one_rows = np.ones((1, np.shape(X)[0] ))\n",
    "    a1 = np.r_[one_rows, X.T]  if X_bias is None else X_bias\n",
    "    z2 = theta1.dot( a1 )\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.r_[one_rows, a2] \n",
    "    z3 = theta2.dot( a2 )\n",
    "    a3 = sigmoid( z3 )\n",
    "#     # Input layer\n",
    "#     m, _ = np.shape(X)\n",
    "#     one_rows = np.ones((1, np.shape(X)[0] ))\n",
    "#     a1 = np.c_[np.ones((m,1)), X] # assigning a1 to X, and adding a bias (m, input_layer_size + 1)\n",
    "#     # Hidden layer\n",
    "#     z2 = a1.dot(theta1.T)\n",
    "#     a2 = sigmoid(z2)\n",
    "#     a2 = np.c_[np.ones((np.shape(a2)[0], 1)), a2] # bias for hidden layer\n",
    "#     # Output layer\n",
    "#     z3 = a2.dot(theta2.T)\n",
    "#     a3 = sigmoid(z3) #a3 = h(x)\n",
    "    \n",
    "    return (a1, a2, a3, z2, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda, yk = None, X_bias = None ):\n",
    "    \n",
    "    theta1, theta2 = param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "    a1,a2,a3,z2,z3 = feed_forward(theta1, theta2, X, X_bias)\n",
    "    \n",
    "    if yk is None:\n",
    "        yk = recode_label(y, num_labels)\n",
    "        assert shape(yk) == shape(a3), \"Error, shape of recoded y is different from a3\"\n",
    "    \n",
    "    # J(theta) function: cross-entropy\n",
    "    term1 = (-y_k * np.log(a3))\n",
    "    term2 = (1 - y_k) * np.log(1 - a3)\n",
    "    cost = np.sum(term1 + term2)/m\n",
    "    # Regularization sum\n",
    "    reg_term = np.sum(theta1 ** 2) + np.sum(theta2[:,1:] ** 2)\n",
    "    reg_term = (_lambda/2/m) * reg_term\n",
    "    return(cost + reg_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda, yk = None, X_bias = None ):\n",
    "    m, n = np.shape(X)\n",
    "    theta1, theta2 = param_unroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "    a1, a2, a3, z2, z3 = feed_forward(theta1, theta2, X)\n",
    "    \n",
    "    # back propagate\n",
    "    if yk is None:\n",
    "        yk = recode_label(y, num_labels )\n",
    "        assert shape(yk) == shape(a3), \"Error: shape of recoded y is different from a3\"\n",
    "    \n",
    "    #size of a3 == size of y_k == (num_labels, m)\n",
    "    delta_3 = a3 - y_k \n",
    "    # check this line\n",
    "    #delta_2 = (delta_3.dot(theta2))[:,1:] * sigmoid_gradient(z2) #ignore bias\n",
    "    delta_2 = theta2.T.dot(delta_3) * sigmoid_gradient(np.r_[np.ones((1, m)), z2 ]) # WHAT\n",
    "    delta_2 = delta_2[1:,:]\n",
    "   \n",
    "    sum_1 = delta_2.dot(a1.T)\n",
    "    sum_2 = delta_3.dot(a2.T) # sum of a_i * delta_i+1\n",
    "    \n",
    "    # putting the gradient equation together\n",
    "    sum_1[:,1:] = (sum_1[:,1:] / m) + (theta1[:,1:] * _lambda / m)\n",
    "    sum_2[:,1:] = (sum_2[:,1:] / m) + (theta2[:,1:] * _lambda / m)\n",
    "    \n",
    "    accum = np.array([sum_1.T.reshape(-1).tolist() + sum_2.T.reshape(-1).tolist()]).T\n",
    "    return np.ndarray.flatten(accum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient( theta, input_layer_size, hidden_layer_size, num_labels, X, y, lamda ):\n",
    "    numgrad = np.zeros( np.shape(theta) )\n",
    "    perturb = np.zeros( np.shape(theta) ) \n",
    "    e = 1e-4\n",
    "\n",
    "    num_elements = np.shape(theta)[0]\n",
    "    yk = recode_label( y, num_labels )\n",
    "\n",
    "    for p in range(0, num_elements) :\n",
    "        perturb[p] = e\n",
    "        loss1 = compute_cost( theta - perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk )\n",
    "        loss2 = compute_cost( theta + perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk )\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * e)\n",
    "        perturb[p] = 0\n",
    "\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated theta: (30, 785)\n",
      "generated theta: (26, 31)\n",
      "shape of y_k: (26, 688)\n",
      "(785, 688)\n"
     ]
    }
   ],
   "source": [
    "# loading input data\n",
    "data = np.genfromtxt('./data/tt.csv', delimiter=',')\n",
    "y = data[:,0].reshape(-1,1)\n",
    "#X = data[:, 1:] / 255.0 #(m, input_layer_size)\n",
    "X = data[:, 1:] #(m, input_layer_size)\n",
    "m = len(y)\n",
    "\n",
    "# Network architecture \n",
    "input_layer_size = 784\n",
    "hidden_layer_size = 30\n",
    "num_labels = 26\n",
    "lam = 1.0\n",
    "\n",
    "# Params\n",
    "theta1 = initialize_weights( 784, 30 ) # (input_size, hidden layer 1 size)\n",
    "theta2 = initialize_weights( 30, 26 )  # (hidden layer 1 size, # labels)\n",
    "unrolled = np.r_[theta1.T.flatten(), theta2.T.flatten()] # 1 dimension (24356,)\n",
    "\n",
    "#grad1 shape: (30, 784) \n",
    "#grad2 shape: (26, 30)\n",
    "\n",
    "\n",
    "# matrix holding correct values\n",
    "y_k = recode_label(y,num_labels)\n",
    "print(f'shape of y_k: {np.shape(y_k)}')\n",
    "\n",
    "X_bias = np.r_[ np.ones((1, np.shape(X)[0] )), X.T] #(input_size + 1, m)\n",
    "print(np.shape(X_bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debugging the parameters before\n",
    "# print(f'rolled params: {np.shape(unrolled)}')\n",
    "# print(f'number of elements in theta1: {np.size(theta1)}')\n",
    "# print(f'number of elements in theta2: {np.size(theta2)}')\n",
    "# print(f'theta1 shape: {np.shape(theta1)}')\n",
    "# print(f'theta2 shape: {np.shape(theta2)}')\n",
    "\n",
    "# print('='*20)\n",
    "\n",
    "# t1,t2 = param_unroll(unrolled, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "# # Debugging after:\n",
    "# print(f'number of elements in theta1: {np.size(t1)}')\n",
    "# print(f'number of elements in theta2: {np.size(t2)}')\n",
    "# print(f'theta1 shape: {np.shape(t1)}')\n",
    "# print(f'theta2 shape: {np.shape(t2)}')\n",
    "\n",
    "# # debugging values are the same\n",
    "# print(f'{theta1[0,0] == t1[0,0]},{theta1[12,25] == t1[12,25]},{theta1[11, 200] == t1[11,200]}')\n",
    "# print(f'{theta2[0,0] == t2[0,0]},{theta2[12,25] == t2[12,25]},{theta2[11, 11] == t2[11,11]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.         ...  0.00037675 -0.0012299\n",
      " -0.00013283]\n"
     ]
    }
   ],
   "source": [
    "# print(compute_cost( unrolled, input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias))\n",
    "# print('=' * 20)\n",
    "# print(numerical_gradient( unrolled, input_layer_size, hidden_layer_size, num_labels, X, y, lam ))\n",
    "# print('=' * 20)\n",
    "print(compute_gradient( unrolled, input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias))\n",
    "# print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimize.fmin_cg(compute_cost, fprime=compute_gradient, x0=unrolled,\n",
    "    args=(input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias),\n",
    "    maxiter=400, disp=True, full_output=True )\n",
    "# result = optimize.fmin_bfgs(compute_cost, fprime=compute_gradient, x0=unrolled,\n",
    "#     args=(input_layer_size, hidden_layer_size, num_labels, X, y, lam, y_k, X_bias),\n",
    "#     maxiter=50, disp=True, full_output=True )\n",
    "\n",
    "# print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta1, theta2 = param_unroll( result[0], input_layer_size, hidden_layer_size, num_labels )\n",
    "# def predict(item, theta1, theta2):\n",
    "#     a1 = np.r_[np.ones((1, 1)), X.reshape(np.shape(X)[0], 1 )]\n",
    "#     z2 = sigmoid( theta1.dot( a1 ))\n",
    "#     z2 = np.r_[np.ones((1, 1)), z2]\n",
    "#     z3 = sigmoid(theta2.dot( z2 ))\n",
    "#     return argmax(z3) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
